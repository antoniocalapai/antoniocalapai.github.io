<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Dr. Antonino Calapai – Software Engineer & Neuroscientist</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta
    name="description"
    content="Dr. Antonino Calapai"
  />
  <link rel="stylesheet" href="styles.css" />
</head>
<script>
function initCarousel(id) {
  const carousel = document.getElementById(id);
  const track = carousel.querySelector('.carousel-images');
  const images = track.querySelectorAll('img');
  let index = 0;

  const update = () => {
    track.style.transform = `translateX(-${index * 100}%)`;
  };

  carousel.querySelector('.prev').onclick = () => {
    index = (index - 1 + images.length) % images.length;
    update();
  };

  carousel.querySelector('.next').onclick = () => {
    index = (index + 1) % images.length;
    update();
  };
}
</script>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <div class="logo">
        <span class="logo-mark"></span>
        <span class="logo-text">Antonino Calapai</span>
      </div>
      <nav class="nav">
        <a href="#about">About</a>
        <a href="#skills">Skills</a>
        <a href="#experience">Experience</a>
        <a href="#projects">Projects</a>
        <a href="#publications">Publications</a>
        <a href="#contact">Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <!-- Hero -->
    <section id="hero" class="hero">
      <div class="container hero-inner">
        <div class="hero-text">
          <p class="hero-kicker"></p>
          <h1>Software Engineer and Neuroscientist.</h1>
          <p class="hero-subtitle">
            I specialize in end-to-end software and data interfaces, autonomous workflows, scientific programming pipelines,
            and animal-computer interactions. I have over a decade of experience building full-stack autonomous devices,
            embedded distributed systems, and machine vision pipelines.
          </p>
          <div class="hero-actions">
            <!-- Update the CV link once your PDF is in the repo -->
            <a class="btn btn-primary" href="#experience">View Experience</a>
            <a class="btn btn-secondary" href="#contact">Contact me</a>
          </div>
        </div>
        <div class="hero-meta">
          <div class="meta-card">
            <p class="meta-label">Location</p>
            <p class="meta-value">Göttingen, Germany (Freiburg from 2026)</p>
          </div>
          <div class="meta-card">
            <p class="meta-label">Current role</p>
            <p class="meta-value">Lead Software Engineer · German Primate Center</p>
          </div>
          <div class="meta-card">
            <p class="meta-label">Focus</p>
            <p class="meta-value">Data engineering · Software Development · Machine vision · Cognitive enrichment</p>
          </div>
        </div>
      </div>
    </section>

    <!-- About -->
    <section id="about" class="section">
      <div class="container section-inner">
        <div class="section-header">
          <h2>About</h2>
<!--          <p class="section-lead">-->
<!--            I am a neuroscientist turned software engineer, building autonomous systems for-->
<!--            studying cognition and behaviour in primates and humans.-->
<!--          </p>-->
        </div>
        <div class="two-column">
          <div>
            <p>
              I was born in Sicily and trained as a neuroscientist and neuropsychologist across Italy, UK, and Germany.
              Over the last decade I have worked at the intersection of cognitive neuroscience, machine vision, and
              software engineering, focusing on advancing knowledge and research methods, efficiently and ethically.
            </p>
            <p>
              During my PhD in Systems Neuroscience at the University of Göttingen, I worked on the electrophysiology
              of the macaque visual cortex and on the psychophysics of spatial attention. Throughout my PostDocs
              I built autonomous systems to turn cognitive training of non-human primates into gamified assessment and enrichment
              that allowed more efficient high-throughput research while enhancing ethical standards.
            </p>
          </div>
          <div>
            <p>
              Today I lead the development of autonomous behavioral tracking and enrichment platforms at the
              Primate Cognition and Behavior facility at the German Primate Center.
            </p>
            <p>
              I enjoy designing systems end-to-end: from CAD models and embedded hardware to data pipelines and user-facing tools for scientists, vets, and technicians.
              My work has led to open-access publications, new research infrastructure, and concrete improvements in animal welfare.
            </p>
            <p>
              Outside of work I am a father of three, an amateur musician, and an aspiring game designer.
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- Skills -->
    <section id="skills" class="section section-alt">
      <div class="container section-inner">
        <div class="section-header">
          <h2> Full-stack developer</h2>
          <p class="section-lead">
            from embedded linux to front-end interfaces - for data science, machine vision, and advanced UX.
          </p>
        </div>
        <div class="grid skills-grid">
          <div class="card">
            <h3>Programming</h3>
            <ul>
              <li>Python</li>
              <li>Matlab</li>
              <li>C# / C++</li>
              <li>Bash</li>
            </ul>
          </div>
          <div class="card">
            <h3>Machine Vision</h3>
            <ul>
              <li>CUDA / MPS</li>
              <li>OpenCV</li>
              <li>TensorFlow</li>
              <li>CNNs, Poses, Identities</li>
              <li>Markerless 3D tracking</li>
            </ul>
          </div>
          <div class="card">
            <h3>Data</h3>
            <ul>
              <li>End-to-end workflows</li>
              <li>GitHub / CI/CD</li>
              <li>Docker & Ansible</li>
              <li>GPU clusters</li>
            </ul>
          </div>
          <div class="card">
            <h3>Hardware</h3>
            <ul>
              <li>Nvidia Jetson</li>
              <li>Raspberry Pi</li>
              <li>PCB basics</li>
              <li>Microcontrollers</li>
              <li>CAD & 3D printing</li>
              <li>Touchscreens</li>

            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- Experience -->
    <section id="experience" class="section">
      <div class="container section-inner">
        <div class="section-header">
          <h2>Experience</h2>
          <p class="section-lead">
            Leading technical teams and infrastructures for animal cognition and machine vision.
          </p>
        </div>
        <div class="timeline">
          <article class="timeline-item">
            <div class="timeline-meta">
              <span class="timeline-role">Lead Machine Vision Engineer</span>
              <span class="timeline-time">2023 – present</span>
            </div>
            <h3>German Primate Center · Göttingen, Germany</h3>
            <p>
              Leading a team of engineers to build full-stack machine vision pipelines for
              <strong>3D tracking and ID recognition of macaques</strong> in a new research
              facility. Responsibilities span system architecture, GPU cluster orchestration,
              and user-facing tools for researchers and animal care staff.
            </p>
          </article>

          <article class="timeline-item">
            <div class="timeline-meta">
              <span class="timeline-role">Postdoctoral Researcher</span>
              <span class="timeline-time">2020 – 2022</span>
            </div>
            <h3>Cognitive Neuroscience Laboratory · German Primate Center</h3>
            <p>
              Developed CNN-based detection systems for autonomous training and assessment of
              monkeys, and designed gamified tasks to increase engagement and data throughput.
              Integrated behavioural, neural, and eye-movement data into unified analysis workflows.
            </p>
          </article>

          <article class="timeline-item">
            <div class="timeline-meta">
              <span class="timeline-role">Junior Postdoctoral Researcher</span>
              <span class="timeline-time">2016 – 2019</span>
            </div>
            <h3>Institute for Auditory Neuroscience · University Medical Center Göttingen</h3>
            <p>
              Built autonomous auditory training and psychophysics systems for common marmosets
              and implemented group-based cognitive training and enrichment protocols for macaques.
            </p>
          </article>

          <article class="timeline-item">
            <div class="timeline-meta">
              <span class="timeline-role">PhD in Systems Neuroscience</span>
              <span class="timeline-time">2011 – 2016</span>
            </div>
            <h3>University of Göttingen · Germany</h3>
            <p>
              Worked on multidimensional mapping of macaque visual cortex (area MST), eye movements
              and visual attention in humans, and early prototypes of cage-based autonomous
              touchscreens for monkeys.
            </p>
          </article>
        </div>
      </div>
    </section>

    <!-- Publications -->
    <section id="publications" class="section">
      <div class="container section-inner">
        <div class="section-header">
          <h2>Selected Publications</h2>
          <p class="section-lead">
            A subset of peer-reviewed work spanning methods, cognition, and animal enrichment.
          </p>
        </div>
        <ul class="pub-list">
          <li>
            <strong>Calapai A. et al. (2022)</strong> –
            Flexible auditory training, psychophysics, and enrichment of common marmosets with an
            automated, touchscreen-based system. <em>Nature Communications</em>.
          </li>
          <li>
            <strong>Calapai A. et al. (2016)</strong> –
            A cage-based training, cognitive testing and enrichment system for rhesus macaques.
            <em>Behavior Research Methods</em>.
          </li>
          <li>
            <strong>Berger M., Calapai A. et al. (2018)</strong> –
            Standardized automated training of rhesus monkeys in their housing environment.
            <em>Journal of Neurophysiology</em>.
          </li>
          <li>
            <strong>Xue C., Calapai A. et al. (2020)</strong> –
            Sustained spatial attention accounts for the direction bias of human microsaccades.
            <em>Scientific Reports</em>.
          </li>
          <li>
            <strong>Cabrera-Moreno J., Calapai A. et al. (2022)</strong> –
            Group-based, autonomous, individualized training and testing of long-tailed macaques
            in their home enclosure. <em>Frontiers in Psychology</em>.
          </li>
          <li>
            Full list available on <a href="https://scholar.google.com/citations?user=lO13KyoAAAAJ&hl=en" target="_blank" rel="noopener">Google Scholar</a>.
          </li>
        </ul>
      </div>
    </section>

    <!-- Projects -->
    <section id="projects" class="section section-alt">
      <div class="container section-inner">

        <div class="section-header">
          <h2>Portfolio</h2>
          <p class="section-lead">
            A selection of projects at the intersection of neuroscience, machine vision, and autonomous systems.
          </p>
        </div>

        <div class="grid projects-grid">

          <!-- ABT -->
          <article class="card">
            <h3>Autonomous Behavioural Tracking (ABT)</h3>

            <div id="carousel-abt" class="carousel">
              <div class="carousel-images">
                <img src="assets/abt_1.jpg" alt="ABT Concept">
                <img src="assets/abt_2.jpg" alt="Elmo spotted by ABT">
                <img src="assets/abt_3.jpg" alt="Joker spotted by ABT">
              </div>
              <button class="carousel-button prev">‹</button>
              <button class="carousel-button next">›</button>
            </div>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              In this projects I am lead developer and coordinator of a group engineers and data scientists
              belonging to different research groups of the Cognitive Neuroscience Laboratory (CNL) of the
              German Primate Center. Here I designed and led the development of a stand-alone software to be
              able to track, identify, and extract the posture of rhesus macaques in their enclosure with
              the aim of autonomously assessing animals’ welfare and behaviour 24/7 without direct interference.
            </p>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              The system coordinates a network of synchronized cameras, GPU-based inference modules,
              and centralized data-management pipelines to support continuous, high-throughput acquisition,
              compression, analysis, and reporting.
            </p>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              High-resolution video streams are captured by up to <strong>16 Hikrobot GigE cameras</strong>,
              connected through a dedicated network switch and synchronized via <strong>PTP</strong> or software
              triggering. These streams are transmitted to a distributed GPU cluster, where a
              <strong>2D Prediction module</strong> performs real-time object detection, keypoint extraction, and
              individual animal identification.
            </p>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              Each node in the cluster employs <strong>TensorRT-optimized CNNs</strong>, trained and fine-tuned
              through custom modules (<em>Model Training</em> and <em>Model Optimization</em>). Every processed frame
              generates structured outputs—including bounding boxes, anatomical keypoints, and identity
              labels—which are automatically logged and archived into text-based databases.
            </p>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              Processed outputs from the GPU cluster are transmitted to a <strong>Server computer</strong>, where
              <em>3D Calibration</em>, <em>3D Transformation</em>, and <em>3D Localization</em> modules reconstruct
              absolute spatial positions from synchronized camera pairs. Calibration follows checkerboard-based
              stereo recordings using the <strong>JARVIS protocol</strong> (jarvis-mocap.github.io), generating
              intrinsic/extrinsic parameters and distortion coefficients for each camera.
            </p>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              Transformation matrices are then computed to project all 2D coordinates into a unified world
              reference frame, enabling accurate <strong>3D pose reconstruction</strong> and <strong>multi-animal
              tracking</strong> across large experimental spaces. The data server manages compression, indexing,
              and long-term storage of video and derivative datasets, ensuring reproducibility and accessibility.
            </p>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              Users interact with the system via the <strong>ABT graphical interface</strong> from a client computer,
              which integrates tools for acquisition scheduling, system monitoring, and visualization of processed
              results. The GUI also links to automated analysis modules that routinely compute behavioral readouts
              and statistical summaries directly from processed data.
            </p>

            <p class="project-description" style="margin-bottom: 1.6rem;">
              This automated architecture is designed to allow researchers to focus on analysis and interpretation,
              while simultaneously enhancing <strong>reproducibility</strong> and <strong>standardization</strong>
              across experimental setups and research questions.
            </p>

            <script>initCarousel("carousel-abt")</script>
          </article>

          <!-- XBI -->
          <article class="card">
            <h3>Cognitive Assessment, enrichment, and training, for Monkeys: the XBI devices </h3>

            <div id="carousel-xbi" class="carousel">
              <div class="carousel-images">
                <img src="assets/XBI_1.jpg" alt="XBI for Rhesus">
                <img src="assets/XBI_2.jpg" alt="XBI for Marmosets">
                <img src="assets/XBI_3.jpg" alt="XBI for Long Tailed">
              </div>
              <button class="carousel-button prev">‹</button>
              <button class="carousel-button next">›</button>
            </div>

            <p class="project-description">
              Here I have focused on designing, developing, and deploying touchscreen devices, code,
              and experimental protocols for captive monkeys (Rhesus macaques, Common Marmosets, Long-tailed macaques).
              The devices are known as Experimental Behavioral Instruments, XBIs in short.
              This approach represents a paradigm shift from traditional chair-seated methods, offering numerous benefits
              for both the animals and the research process, and as a consequence it has received the support of the
              scientific community. Through these devices and activities, captive animals can access cognitive training
              and enrichment directly from their home enclosure. This not only reduces potential stress associated with
              movement constraints but also provides cognitive assessment data.
              From a practical standpoint, this system enhances the way cognitive research can be conducted.
              By automating the data acquisition process, I have contributed to streamlining the research workflow,
              saving considerable time and resources while improving animal welfare. This automation allows for
              scaling up the number of animals tested simultaneously, addressing previous ethical, practical,
              and scientific limitations in cognitive research with primates
            </p>

            <script>initCarousel("carousel-xbi")</script>
          </article>

          <!-- Neurons -->
          <article class="card">
            <h3>Neuronal Encoding of Motion and Depth</h3>

            <div id="carousel-neuro" class="carousel">
              <div class="carousel-images">
                <img src="assets/neu_1.jpg" alt="">
                <img src="assets/neu_2.jpg" alt="">
                <img src="assets/neu_3.jpg" alt="">
              </div>
              <button class="carousel-button prev">‹</button>
              <button class="carousel-button next">›</button>
            </div>

            <p class="project-description">
              As part of my electrophysiology work, I investigated how single neurons in the macaque
              Middle Superior Temporal area (MST) encode <strong>motion</strong> and <strong>depth</strong> information.
              Monkeys were trained to fixate a central point, press a button for a dimming target, and wear
              polarised glasses allowing presentation of stimuli at different depths.
            </p>

            <p class="project-description">
              While the animal performed this simple task, I recorded extracellular activity from MST neurons
              as clouds of moving dots varied rapidly in both motion direction and binocular disparity. This
              allowed me to map each neuron's receptive field and quantify its joint selectivity to motion
              and depth using <strong>Reverse Correlation</strong>.
            </p>

            <p class="project-description">
              Theoretical work suggested that some MST neurons might act as <strong>self-motion detectors</strong>,
              responding to specific combinations of direction and depth. In practice, I found a diversity
              of tuning profiles, including neurons selective for:
            </p>

            <ul style="margin-left: 1.2rem; margin-bottom: 1.2rem; color: var(--fg-muted);">
              <li>specific motion–depth combinations,</li>
              <li>motion only,</li>
              <li>depth only,</li>
              <li>or mixed/complex patterns.</li>
            </ul>

            <p class="project-description">
              Using <strong>Generalised Additive Models (GAMs)</strong> on 194 neurons, only <strong>one</strong> neuron
              showed tuning consistent with a true self-motion detector. Most neurons combined motion and
              depth in ways that provided the <em>building blocks</em> for self-motion computation, but not the
              complete computation itself.
            </p>

            <p class="project-description">
              These results indicate that MST does not explicitly encode self-motion, but instead provides
              intermediate representations that are likely integrated <strong>downstream</strong> to support
              perception of self-movement through space.
            </p>

            <script>initCarousel("carousel-neuro")</script>
          </article>

          <!-- Animal Identification -->
          <article class="card">
          <h3>Machine Vision for Monkeys Identification</h3>

          <div id="carousel-mvi" class="carousel">
            <div class="carousel-images">
              <img src="assets/mvi_1.jpg" alt="">
              <img src="assets/mvi_2.jpg" alt="">
              <img src="assets/mvi_3.jpg" alt="">
              <img src="assets/mvi_4.jpg" alt="">
            </div>
            <button class="carousel-button prev">‹</button>
            <button class="carousel-button next">›</button>
          </div>

          <p class="project-description">
            Since 2020, as part of my transition from neuroscience to software engineering, I have
            integrated machine-learning approaches into several projects to solve three crucial challenges:
          </p>

          <ul style="margin-left: 1.2rem; margin-bottom: 1.2rem; color: var(--fg-muted);">
            <li>identifying which animal is operating a touchscreen at any moment, enabling standardised yet individualised training and enrichment (Berger et al. 2018; Calapai et al. 2022; Cabrera-Moreno et al. 2022);</li>
            <li>estimating gaze direction in real time while animals interact with touchscreen devices;</li>
            <li>autonomously extracting location, identity, posture, and behaviour from video recordings with minimal human supervision.</li>
          </ul>

          <h4 style="margin-top:1.4rem;">Animal identification: real-time, low-latency CNN model</h4>

          <p class="project-description">
            To identify animals in real time, I trained a convolutional neural network in TensorFlow
            (Python 3.7). The model consisted of:
          </p>

          <ul style="margin-left: 1.2rem; margin-bottom: 1.2rem; color: var(--fg-muted);">
            <li>an average pooling input layer (6×3 kernel),</li>
            <li>three convolutional layers (3×3 kernels; 64, 16, 32 units),</li>
            <li>max-pooling, dropout, and flatten layers,</li>
            <li>a dense hidden layer (ReLU),</li>
            <li>a softmax output layer including an additional “null” class for accidental triggers.</li>
          </ul>

          <p class="project-description">
            The network was trained for 10 epochs (batch size 32) using an <em>adam</em> optimizer, sparse
            categorical cross-entropy, and accuracy as metric. Ground truth for the 40k training images
            was created through a custom MATLAB GUI capable of labelling ~50 images per minute.
          </p>

          <p class="project-description">
            This approach was applied to 11 animals across four housing groups (long-tailed and rhesus
            macaques). Across >30k evaluations, the framework and training method had a much larger impact
            on accuracy than the number of animals per network: the TensorFlow/Keras model I designed
            significantly outperformed an equivalent CoreML model trained as a black box on a MacBook Pro.
          </p>

          <p class="project-description">
            The best model (Keras) reached <strong>97.5% identification accuracy</strong> across 8 test animals and
            40k images. The end-to-end processing time—from capturing the image to returning the identity
            and loading the correct training target—was <strong>176 ms</strong>, enabling genuine real-time use.
          </p>

          <p class="project-description">
            In my 2022 publication (Cabrera-Moreno et al.), this method was used in real time to implement
            individualised, step-wise training for a visuo-acoustic discrimination task with long-tailed
            macaques.
          </p>

          <h4 style="margin-top:1.4rem;">Automated hyperparameter search</h4>

          <p class="project-description">
            To identify the optimal network structure, I implemented an automated bootstrap procedure
            (executed on Google Colab with GPU) exploring 46 parameter combinations, including:
          </p>

          <ul style="margin-left: 1.2rem; margin-bottom: 1.2rem; color: var(--fg-muted);">
            <li>average pooling kernel size,</li>
            <li>number of units in the three convolutional layers,</li>
            <li>number of units in the dense hidden layer.</li>
          </ul>

          <p class="project-description">
            The best configuration achieved <strong>98.7% accuracy</strong> in a two-animal test dataset and was used
            during the actual experiment.
          </p>

          <p class="project-description">
            This work is fully described in the peer-reviewed publication by Cabrera-Moreno et al. (2022).
          </p>

          <h4 style="margin-top:1.4rem;">Video-based gaze estimation</h4>

          <p class="project-description">
            In ongoing work, I apply DeepLabCut and custom machine-learning pipelines to detect
            rhesus macaque and marmoset eyes in real time during touchscreen-based cognitive tasks.
            The aim is to integrate <strong>gaze estimation</strong> directly into behavioural training, assessment,
            and enrichment routines.
          </p>

          <p class="project-description">
            (Work in progress — please feel free to contact me for details.)
          </p>

          <script>initCarousel("carousel-mvi")</script>
        </article>

          <!-- Gamified Psychophysics -->
        <article class="card">
          <h3>Gamified Psychophysics</h3>

          <div id="carousel-gam" class="carousel">
            <div class="carousel-images">
              <img src="assets/gam_1.jpg" alt="">
              <img src="assets/gam_2.jpg" alt="">
              <img src="assets/gam_3.jpg" alt="">
            </div>
            <button class="carousel-button prev">‹</button>
            <button class="carousel-button next">›</button>
          </div>

          <p class="project-description">
            Since 2022 I am designing, programming, and testing psychophysical experiments
            that incorporate gaming elements, to study Cognition, Motion Processing,
            Decision-Making, and Integration of Social Information.
          </p>

          <p class="project-description">
            Four major projects sparked from this effort:
          </p>

          <ul style="margin-left:1.2rem; margin-bottom:1.2rem; color:var(--fg-muted);">
            <li>An investigation on the validity of gamified psychophysics for the study of motion processing;</li>
            <li>A dyadic, joystick-based game to study the integration of social information in decision-making;</li>
            <li>A multiple-choice user interface optimised for cognitive assessment, enrichment, and psychophysics in monkeys;</li>
            <li>A foraging-like game to study high-level cognitive flexibility in monkeys and humans.</li>
          </ul>

          <h4 style="margin-top:1.4rem;">Validity of gamified psychophysics for motion processing</h4>

          <p class="project-description">
            Across three experimental conditions with gradually different levels of gamification
            (Standard, Gamified, Unreal Engine), participants played the exact same motion-perception
            task. Gamification increased satisfaction, reduced dropout rates, and improved motion sensitivity,
            especially in the Unreal Engine condition.
          </p>

          <p class="project-description">
            Participants controlled an avatar that moved left/right by following the global motion of
            a cloud of dots. Obstacles appeared too quickly to react, and could be avoided only by
            correctly interpreting motion direction. Points, levels, and dynamic backgrounds supported
            engagement without compromising psychophysical rigor.
          </p>

          <h4 style="margin-top:1.4rem;">Dyadic joystick game for social-information integration</h4>

          <p class="project-description">
            This task evaluated whether humans integrate social motion cues during decision-making.
            Two players simultaneously navigated using joysticks around a cloud of dots whose motion
            predicted where targets would appear. Each participant saw their own arc and the other
            player’s arc. Data analysis is ongoing and cannot be disclosed publicly yet.
          </p>

          <h4 style="margin-top:1.4rem;">Multiple-choice interface (“Task Bar”) for monkeys</h4>

          <p class="project-description">
            This system, published in 2023 (Animals), allows monkeys to select cognitive enrichment
            tasks from their enclosure — a “dock bar” with three buttons:
          </p>

          <ul style="margin-left:1.2rem; margin-bottom:1.2rem; color:var(--fg-muted);">
            <li>Static reach: touch a red ball</li>
            <li>Dynamic reach: touch a moving red ball</li>
            <li>Watch pictures of conspecifics</li>
          </ul>

          <p class="project-description">
            Monkeys reliably chose their preferred tasks, and button repositioning every hour did not
            affect their preferences. Performance metrics (hit rates across size/speed combinations)
            revealed consistent psychophysical signatures.
          </p>

          <h4 style="margin-top:1.4rem;">Foraging-like cognitive flexibility task</h4>

          <p class="project-description">
            Published in 2022 (Frontiers in Psychology), this complex task assessed rule learning and
            switching in monkeys and humans (including adults and children with/without ADHD). Animals
            played thousands of trials per session with strong engagement.
          </p>

          <p class="project-description">
            Results show that humans are more flexible and abstract in rule learning, but monkeys display
            stable individual styles and domain-general rule learning that does not depend on visual category.
          </p>

          <script>initCarousel("carousel-gam")</script>
        </article>

          <!-- Acoustic Perception -->
        <article class="card">
          <h3>Acoustic Perception in Marmosets</h3>

          <div id="carousel-aco" class="carousel">
            <div class="carousel-images">
              <img src="assets/aco_1.jpg" alt="">
              <img src="assets/aco_2.jpg" alt="">
              <img src="assets/aco_3.jpg" alt="">
            </div>
            <button class="carousel-button prev">‹</button>
            <button class="carousel-button next">›</button>
          </div>

          <p class="project-description">
            This project explored how natural, socially meaningful sounds can improve learning and
            engagement in cognitive training. Using a fully autonomous touchscreen system designed
            for common marmosets (MXBI), we showed that ecologically valid acoustic cues such as
            conspecific calls lead to faster learning than artificial tones.
          </p>

          <p class="project-description">
            The MXBI system operates directly in home enclosures, where animals voluntarily interact
            with sound-based tasks identified via RFID. The system adapts task difficulty in real time
            and removes the need for social isolation or food deprivation, turning cognitive testing
            into an enrichment activity aligned with natural behaviour.
          </p>

          <p class="project-description">
            The key result: ecological relevance matters. Marmosets learn faster, stay engaged longer,
            and perform better when acoustic cues match their natural perceptual world.
          </p>

          <h4 style="margin-top:1.4rem;">Figure explanation: learning and motivation</h4>

          <p class="project-description">
            Over repeated sessions, animals consistently improved their ability to associate specific
            acoustic cues with visual choices, performing hundreds of voluntary trials per day without
            supervision. Natural sounds evoked stronger curiosity and sustained motivation compared to
            artificial tones or noise stimuli.
          </p>

          <h4 style="margin-top:1.4rem;">Hearing thresholds</h4>

          <p class="project-description">
            Using an adaptive reward scheme across sound intensities, marmosets’ hearing thresholds were
            automatically estimated in the MXBI system. Psychometric curves revealed thresholds around
            27–49 dB SPL and matched known species-specific hearing ranges.
          </p>

          <h4 style="margin-top:1.4rem;">Sound categories and discrimination</h4>

          <p class="project-description">
            Marmosets discriminated between five sound categories:
          </p>

          <ul style="margin-left:1.2rem; margin-bottom:1.2rem; color:var(--fg-muted);">
            <li>Juvenile calls</li>
            <li>Twitter calls</li>
            <li>Phee calls</li>
            <li>Sine tones</li>
            <li>White noise</li>
          </ul>

          <p class="project-description">
            Natural calls produced faster learning and higher accuracy than artificial sounds. Sensitivity
            measures and hit rates both confirmed superior performance with biologically meaningful stimuli.
          </p>

          <p class="project-description">
            As first author, I developed the hardware, software architecture, adaptive algorithms, and
            behavioural validation framework. The MXBI platform shows that auditory cognition can be
            assessed ethically and fully autonomously inside social enclosures.
          </p>

          <script>initCarousel("carousel-aco")</script>
        </article>

        </div>
      </div>
    </section>

    <!-- Contact -->
    <section id="contact" class="section section-alt">
      <div class="container section-inner">
        <div class="section-header">
          <h2>Contact</h2>
          <p class="section-lead">
            Open to roles in software or data engineering, applied machine learning, scientific programming, around the
            area of Freiburg, Germany.
          </p>
        </div>
        <div class="contact-grid">
          <div>
            <ul class="contact-list">
              <li><strong>Email:</strong> <a href="mailto:antonio.calapai@gmail.com">antonio.calapai@gmail.com</a></li>
              <li><strong>LinkedIn:</strong> <a href="https://www.linkedin.com/in/antonino-calapai-6b8338266" target="_blank" rel="noopener">linkedin.com/in/antonino-calapai-6b8338266</a></li>
              <li><strong>GitHub:</strong> <a href="https://github.com/antoniocalapai" target="_blank" rel="noopener">github.com/antoniocalapai</a></li>
              <li><strong>Location:</strong> Göttingen → Freiburg (Germany)</li>
            </ul>
          </div>
          <div>
            <h3>Current interests</h3>
            <ul class="pill-list">
              <li>Data & ML engineering roles</li>
              <li>Machine vision & tracking</li>
              <li>Cognitive enrichment platforms</li>
              <li>Robotics & interactive systems</li>
            </ul>
          </div>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container footer-inner">
      <p>© <span id="year"></span> Antonino Calapai</p>
      <p>Built with plain HTML &amp; CSS · Hosted on GitHub Pages</p>
    </div>
    <script>
      // Simple year update, no dependencies
      document.getElementById('year').textContent = new Date().getFullYear();
    </script>
  </footer>
</body>
</html>